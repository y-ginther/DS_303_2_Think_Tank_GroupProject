{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test1 = pd.read_csv('data/test1.csv')\n",
    "test2 = pd.read_csv('data/test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>HOME_TEAM_WINS</th>\n",
       "      <th>HOME_TEAM_PTS_AVG</th>\n",
       "      <th>AWAY_TEAM_PTS_AVG</th>\n",
       "      <th>HOME_TEAM_AST_AVG</th>\n",
       "      <th>AWAY_TEAM_AST_AVG</th>\n",
       "      <th>HOME_TEAM_REB_AVG</th>\n",
       "      <th>AWAY_TEAM_REB_AVG</th>\n",
       "      <th>HOME_TEAM_WIN_PCT</th>\n",
       "      <th>AWAY_TEAM_WIN_PCT</th>\n",
       "      <th>...</th>\n",
       "      <th>AWAY_TEAM_FT_PCT_AVG</th>\n",
       "      <th>HOME_TEAM_FG3_PCT_AVG</th>\n",
       "      <th>AWAY_TEAM_FG3_PCT_AVG</th>\n",
       "      <th>DIFF_PTS_AVG</th>\n",
       "      <th>DIFF_AST_AVG</th>\n",
       "      <th>DIFF_REB_AVG</th>\n",
       "      <th>DIFF_WIN_PCT</th>\n",
       "      <th>DIFF_FG_PCT</th>\n",
       "      <th>DIFF_FT_PCT</th>\n",
       "      <th>DIFF_FG3_PCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5881</td>\n",
       "      <td>0</td>\n",
       "      <td>90.437500</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>19.312500</td>\n",
       "      <td>20.875000</td>\n",
       "      <td>40.187500</td>\n",
       "      <td>39.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756938</td>\n",
       "      <td>0.330500</td>\n",
       "      <td>0.379437</td>\n",
       "      <td>-0.812500</td>\n",
       "      <td>-1.562500</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>-0.034437</td>\n",
       "      <td>-0.046063</td>\n",
       "      <td>-0.048938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5882</td>\n",
       "      <td>1</td>\n",
       "      <td>99.933333</td>\n",
       "      <td>97.444444</td>\n",
       "      <td>21.666667</td>\n",
       "      <td>18.277778</td>\n",
       "      <td>46.266667</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.782056</td>\n",
       "      <td>0.347533</td>\n",
       "      <td>0.343611</td>\n",
       "      <td>2.488889</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>2.266667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.029189</td>\n",
       "      <td>-0.021456</td>\n",
       "      <td>0.003922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5883</td>\n",
       "      <td>1</td>\n",
       "      <td>100.647059</td>\n",
       "      <td>92.526316</td>\n",
       "      <td>21.470588</td>\n",
       "      <td>20.736842</td>\n",
       "      <td>43.529412</td>\n",
       "      <td>42.473684</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.733474</td>\n",
       "      <td>0.308765</td>\n",
       "      <td>0.334737</td>\n",
       "      <td>8.120743</td>\n",
       "      <td>0.733746</td>\n",
       "      <td>1.055728</td>\n",
       "      <td>0.390093</td>\n",
       "      <td>0.029362</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>-0.025972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5884</td>\n",
       "      <td>1</td>\n",
       "      <td>98.411765</td>\n",
       "      <td>99.352941</td>\n",
       "      <td>21.529412</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>47.352941</td>\n",
       "      <td>39.470588</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737824</td>\n",
       "      <td>0.313882</td>\n",
       "      <td>0.405235</td>\n",
       "      <td>-0.941176</td>\n",
       "      <td>1.764706</td>\n",
       "      <td>7.882353</td>\n",
       "      <td>-0.176471</td>\n",
       "      <td>-0.020647</td>\n",
       "      <td>0.037412</td>\n",
       "      <td>-0.091353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5885</td>\n",
       "      <td>1</td>\n",
       "      <td>91.888889</td>\n",
       "      <td>97.421053</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>41.722222</td>\n",
       "      <td>39.894737</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785211</td>\n",
       "      <td>0.310667</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>-5.532164</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>1.827485</td>\n",
       "      <td>0.014620</td>\n",
       "      <td>-0.034640</td>\n",
       "      <td>-0.064544</td>\n",
       "      <td>-0.001333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>4201</td>\n",
       "      <td>1</td>\n",
       "      <td>95.653465</td>\n",
       "      <td>95.390000</td>\n",
       "      <td>21.574257</td>\n",
       "      <td>20.480000</td>\n",
       "      <td>40.475248</td>\n",
       "      <td>43.780000</td>\n",
       "      <td>0.663366</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.694590</td>\n",
       "      <td>0.344158</td>\n",
       "      <td>0.337540</td>\n",
       "      <td>0.263465</td>\n",
       "      <td>1.094257</td>\n",
       "      <td>-3.304752</td>\n",
       "      <td>0.073366</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>0.063479</td>\n",
       "      <td>0.006618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>4202</td>\n",
       "      <td>1</td>\n",
       "      <td>101.288462</td>\n",
       "      <td>97.920000</td>\n",
       "      <td>24.173077</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>43.144231</td>\n",
       "      <td>40.690000</td>\n",
       "      <td>0.605769</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754910</td>\n",
       "      <td>0.333856</td>\n",
       "      <td>0.378780</td>\n",
       "      <td>3.368462</td>\n",
       "      <td>2.193077</td>\n",
       "      <td>2.454231</td>\n",
       "      <td>-0.094231</td>\n",
       "      <td>-0.002543</td>\n",
       "      <td>-0.011593</td>\n",
       "      <td>-0.044924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2978</th>\n",
       "      <td>4203</td>\n",
       "      <td>1</td>\n",
       "      <td>95.198020</td>\n",
       "      <td>95.490196</td>\n",
       "      <td>20.445545</td>\n",
       "      <td>21.588235</td>\n",
       "      <td>43.732673</td>\n",
       "      <td>40.441176</td>\n",
       "      <td>0.584158</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755284</td>\n",
       "      <td>0.337327</td>\n",
       "      <td>0.346127</td>\n",
       "      <td>-0.292176</td>\n",
       "      <td>-1.142691</td>\n",
       "      <td>3.291497</td>\n",
       "      <td>-0.082508</td>\n",
       "      <td>-0.008960</td>\n",
       "      <td>-0.060967</td>\n",
       "      <td>-0.008801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>4204</td>\n",
       "      <td>0</td>\n",
       "      <td>101.361905</td>\n",
       "      <td>97.772277</td>\n",
       "      <td>24.190476</td>\n",
       "      <td>21.930693</td>\n",
       "      <td>43.028571</td>\n",
       "      <td>40.554455</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.693069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755356</td>\n",
       "      <td>0.337029</td>\n",
       "      <td>0.378327</td>\n",
       "      <td>3.589628</td>\n",
       "      <td>2.259783</td>\n",
       "      <td>2.474116</td>\n",
       "      <td>-0.083545</td>\n",
       "      <td>-0.001611</td>\n",
       "      <td>-0.012871</td>\n",
       "      <td>-0.041298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>4205</td>\n",
       "      <td>1</td>\n",
       "      <td>95.127451</td>\n",
       "      <td>95.359223</td>\n",
       "      <td>20.441176</td>\n",
       "      <td>21.524272</td>\n",
       "      <td>43.637255</td>\n",
       "      <td>40.378641</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756204</td>\n",
       "      <td>0.338549</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>-0.231772</td>\n",
       "      <td>-1.083095</td>\n",
       "      <td>3.258614</td>\n",
       "      <td>-0.071959</td>\n",
       "      <td>-0.008431</td>\n",
       "      <td>-0.061223</td>\n",
       "      <td>-0.007451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2981 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  HOME_TEAM_WINS  HOME_TEAM_PTS_AVG  AWAY_TEAM_PTS_AVG  \\\n",
       "0           5881               0          90.437500          91.250000   \n",
       "1           5882               1          99.933333          97.444444   \n",
       "2           5883               1         100.647059          92.526316   \n",
       "3           5884               1          98.411765          99.352941   \n",
       "4           5885               1          91.888889          97.421053   \n",
       "...          ...             ...                ...                ...   \n",
       "2976        4201               1          95.653465          95.390000   \n",
       "2977        4202               1         101.288462          97.920000   \n",
       "2978        4203               1          95.198020          95.490196   \n",
       "2979        4204               0         101.361905          97.772277   \n",
       "2980        4205               1          95.127451          95.359223   \n",
       "\n",
       "      HOME_TEAM_AST_AVG  AWAY_TEAM_AST_AVG  HOME_TEAM_REB_AVG  \\\n",
       "0             19.312500          20.875000          40.187500   \n",
       "1             21.666667          18.277778          46.266667   \n",
       "2             21.470588          20.736842          43.529412   \n",
       "3             21.529412          19.764706          47.352941   \n",
       "4             16.000000          19.000000          41.722222   \n",
       "...                 ...                ...                ...   \n",
       "2976          21.574257          20.480000          40.475248   \n",
       "2977          24.173077          21.980000          43.144231   \n",
       "2978          20.445545          21.588235          43.732673   \n",
       "2979          24.190476          21.930693          43.028571   \n",
       "2980          20.441176          21.524272          43.637255   \n",
       "\n",
       "      AWAY_TEAM_REB_AVG  HOME_TEAM_WIN_PCT  AWAY_TEAM_WIN_PCT  ...  \\\n",
       "0             39.375000           0.250000           0.437500  ...   \n",
       "1             44.000000           0.800000           0.500000  ...   \n",
       "2             42.473684           0.705882           0.315789  ...   \n",
       "3             39.470588           0.470588           0.647059  ...   \n",
       "4             39.894737           0.277778           0.263158  ...   \n",
       "...                 ...                ...                ...  ...   \n",
       "2976          43.780000           0.663366           0.590000  ...   \n",
       "2977          40.690000           0.605769           0.700000  ...   \n",
       "2978          40.441176           0.584158           0.666667  ...   \n",
       "2979          40.554455           0.609524           0.693069  ...   \n",
       "2980          40.378641           0.588235           0.660194  ...   \n",
       "\n",
       "      AWAY_TEAM_FT_PCT_AVG  HOME_TEAM_FG3_PCT_AVG  AWAY_TEAM_FG3_PCT_AVG  \\\n",
       "0                 0.756938               0.330500               0.379437   \n",
       "1                 0.782056               0.347533               0.343611   \n",
       "2                 0.733474               0.308765               0.334737   \n",
       "3                 0.737824               0.313882               0.405235   \n",
       "4                 0.785211               0.310667               0.312000   \n",
       "...                    ...                    ...                    ...   \n",
       "2976              0.694590               0.344158               0.337540   \n",
       "2977              0.754910               0.333856               0.378780   \n",
       "2978              0.755284               0.337327               0.346127   \n",
       "2979              0.755356               0.337029               0.378327   \n",
       "2980              0.756204               0.338549               0.346000   \n",
       "\n",
       "      DIFF_PTS_AVG  DIFF_AST_AVG  DIFF_REB_AVG  DIFF_WIN_PCT  DIFF_FG_PCT  \\\n",
       "0        -0.812500     -1.562500      0.812500     -0.187500    -0.034437   \n",
       "1         2.488889      3.388889      2.266667      0.300000     0.029189   \n",
       "2         8.120743      0.733746      1.055728      0.390093     0.029362   \n",
       "3        -0.941176      1.764706      7.882353     -0.176471    -0.020647   \n",
       "4        -5.532164     -3.000000      1.827485      0.014620    -0.034640   \n",
       "...            ...           ...           ...           ...          ...   \n",
       "2976      0.263465      1.094257     -3.304752      0.073366     0.008609   \n",
       "2977      3.368462      2.193077      2.454231     -0.094231    -0.002543   \n",
       "2978     -0.292176     -1.142691      3.291497     -0.082508    -0.008960   \n",
       "2979      3.589628      2.259783      2.474116     -0.083545    -0.001611   \n",
       "2980     -0.231772     -1.083095      3.258614     -0.071959    -0.008431   \n",
       "\n",
       "      DIFF_FT_PCT  DIFF_FG3_PCT  \n",
       "0       -0.046063     -0.048938  \n",
       "1       -0.021456      0.003922  \n",
       "2        0.000703     -0.025972  \n",
       "3        0.037412     -0.091353  \n",
       "4       -0.064544     -0.001333  \n",
       "...           ...           ...  \n",
       "2976     0.063479      0.006618  \n",
       "2977    -0.011593     -0.044924  \n",
       "2978    -0.060967     -0.008801  \n",
       "2979    -0.012871     -0.041298  \n",
       "2980    -0.061223     -0.007451  \n",
       "\n",
       "[2981 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.HOME_TEAM_WINS\n",
    "X = train.drop(\"HOME_TEAM_WINS\", axis = 1)\n",
    "\n",
    "y_test = test1.HOME_TEAM_WINS\n",
    "X_test = test1.drop(\"HOME_TEAM_WINS\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'HOME_TEAM_PTS_AVG', 'AWAY_TEAM_PTS_AVG',\n",
       "       'HOME_TEAM_AST_AVG', 'AWAY_TEAM_AST_AVG', 'HOME_TEAM_REB_AVG',\n",
       "       'AWAY_TEAM_REB_AVG', 'HOME_TEAM_WIN_PCT', 'AWAY_TEAM_WIN_PCT',\n",
       "       'HOME_TEAM_FG_PCT_AVG', 'AWAY_TEAM_FG_PCT_AVG', 'HOME_TEAM_FT_PCT_AVG',\n",
       "       'AWAY_TEAM_FT_PCT_AVG', 'HOME_TEAM_FG3_PCT_AVG',\n",
       "       'AWAY_TEAM_FG3_PCT_AVG', 'DIFF_PTS_AVG', 'DIFF_AST_AVG', 'DIFF_REB_AVG',\n",
       "       'DIFF_WIN_PCT', 'DIFF_FG_PCT', 'DIFF_FT_PCT', 'DIFF_FG3_PCT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sklearn_model = LogisticRegression( max_iter = 1000)\n",
    "selected_features_1 = ['DIFF_WIN_PCT', 'DIFF_FG_PCT', 'DIFF_FG3_PCT', 'DIFF_AST_AVG']\n",
    "sklearn_model.fit(X[selected_features_1], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression on test set : 0.6504\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = sklearn_model.predict(test1[selected_features_1])\n",
    "print('Accuracy of logistic regression on test set : {:.4f}'.format(sklearn_model.score(X_test[selected_features_1], y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression on test set : 0.6472\n"
     ]
    }
   ],
   "source": [
    "#after step-feature selection\n",
    "selected_features_1 = ['DIFF_WIN_PCT', 'HOME_TEAM_FG_PCT_AVG', 'DIFF_REB_AVG', 'DIFF_PTS_AVG']\n",
    "sklearn_model.fit(X[selected_features_1], y)\n",
    "y_pred_test = sklearn_model.predict(test1[selected_features_1])\n",
    "print('Accuracy of logistic regression on test set : {:.4f}'.format(sklearn_model.score(X_test[selected_features_1], y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591628\n",
      "         Iterations 6\n",
      "Classification accuracy = 63.7%\n",
      "Precision = 68.8%\n",
      "TPR or Recall = 76.5%\n",
      "FNR = 23.5%\n",
      "FPR = 57.6%\n",
      "ROC-AUC = 64.9%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "model = sm.logit(formula = 'HOME_TEAM_WINS~DIFF_WIN_PCT+HOME_TEAM_FG_PCT_AVG+DIFF_REB_AVG+DIFF_PTS_AVG', data = train).fit()\n",
    "\n",
    "#Function to compute confusion matrix and prediction accuracy on test/train data\n",
    "def confusion_matrix_data(data,actual_values,model,cutoff=0.5):\n",
    "#Predict the values using the Logit model\n",
    "    pred_values = model.predict(data)\n",
    "# Specify the bins\n",
    "    bins=np.array([0,cutoff,1])\n",
    "#Confusion matrix\n",
    "    cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n",
    "    cm_df = pd.DataFrame(cm)\n",
    "    cm_df.columns = ['Predicted 0','Predicted 1']\n",
    "    cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n",
    "# Calculate the accuracy\n",
    "    accuracy = (cm[0,0]+cm[1,1])/cm.sum()\n",
    "    fnr = (cm[1,0])/(cm[1,0]+cm[1,1])\n",
    "    precision = (cm[1,1])/(cm[0,1]+cm[1,1])\n",
    "    fpr = (cm[0,1])/(cm[0,0]+cm[0,1])\n",
    "    tpr = (cm[1,1])/(cm[1,0]+cm[1,1])\n",
    "    fpr_roc, tpr_roc, auc_thresholds = roc_curve(actual_values, pred_values)\n",
    "    auc_value = (auc(fpr_roc, tpr_roc))# AUC of ROC\n",
    "    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.ylabel(\"Actual Values\")\n",
    "    plt.xlabel(\"Predicted Values\")\n",
    "    print(\"Classification accuracy = {:.1%}\".format(accuracy))\n",
    "    print(\"Precision = {:.1%}\".format(precision))\n",
    "    print(\"TPR or Recall = {:.1%}\".format(tpr))\n",
    "    print(\"FNR = {:.1%}\".format(fnr))\n",
    "    print(\"FPR = {:.1%}\".format(fpr))\n",
    "    print(\"ROC-AUC = {:.1%}\".format(auc_value))\n",
    "\n",
    "confusion_matrix_data(test1,test1.HOME_TEAM_WINS,model,cutoff=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to develop a model based on all predictors in predictor_subset\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "def processSubset(predictor_subset):\n",
    "    # Fit model on feature_set and calculate R-squared\n",
    "    model = sm.logit('HOME_TEAM_WINS~' + '+'.join(predictor_subset),data = train).fit()\n",
    "    ypred = model.predict(train)\n",
    "    y=train.HOME_TEAM_WINS\n",
    "    fpr, tpr, auc_thresholds = roc_curve(y, ypred)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    return {\"model\":model, \"auc_roc\":auc_roc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find the best predictor out of p-k predictors and add it to the model containing the k predictors\n",
    "def forward(predictors):\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in X.columns if p not in predictors]\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p]))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['auc_roc'].argmax()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection():\n",
    "    models_best = pd.DataFrame(columns=[\"auc_roc\", \"model\"])\n",
    "\n",
    "    tic = time.time()\n",
    "    \n",
    "    predictors = []\n",
    "\n",
    "    for i in range(1,len(X.columns)+1):    \n",
    "        models_best.loc[i] = forward(predictors)\n",
    "        predictors = list(models_best.loc[i][\"model\"].params.index[1:])\n",
    "\n",
    "    toc = time.time()\n",
    "    print(\"Total elapsed time:\", (toc-tic), \"seconds.\")\n",
    "    return models_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.671289\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.662699\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.668593\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.660647\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.667509\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.666973\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.668776\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.628480\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.639611\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.652314\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.663206\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.670464\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.671250\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.661490\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.663629\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.660561\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.657551\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.664434\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594509\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.643176\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.670645\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.653444\n",
      "         Iterations 5\n",
      "Processed  22 models on 1 predictors in 0.2784590721130371 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594509\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594252\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593689\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593859\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594222\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594063\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594394\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594152\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594152\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593456\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593787\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594509\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594024\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594463\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594463\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594417\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594472\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593974\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594490\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594266\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594509\n",
      "         Iterations 5\n",
      "Processed  21 models on 2 predictors in 0.32361674308776855 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593456\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593439\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592938\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593347\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593327\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592771\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593296\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593444\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593444\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593132\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593443\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593083\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593428\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593455\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593046\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593452\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592655\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593132\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593179\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593437\n",
      "         Iterations 6\n",
      "Processed  20 models on 3 predictors in 0.432966947555542 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592655\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592431\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591893\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592623\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592515\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592571\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592571\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592655\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592655\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592502\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592652\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592405\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592651\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592622\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591628\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592625\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592502\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592490\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592646\n",
      "         Iterations 6\n",
      "Processed  19 models on 4 predictors in 0.42119598388671875 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591628\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591608\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591608\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591490\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591617\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591538\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591538\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591586\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591586\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591598\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591623\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591529\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591623\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591474\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591506\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591598\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591595\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591566\n",
      "         Iterations 6\n",
      "Processed  18 models on 5 predictors in 0.5704786777496338 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591490\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591490\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591490\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591469\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591444\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591444\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591413\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591413\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591441\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591489\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591389\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591477\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591318\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591469\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591441\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591447\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591436\n",
      "         Iterations 6\n",
      "Processed  17 models on 6 predictors in 0.4811220169067383 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591318\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591314\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591314\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591310\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591279\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591279\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591291\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591291\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591300\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591311\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591126\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591314\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591310\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591300\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591253\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591314\n",
      "         Iterations 6\n",
      "Processed  16 models on 7 predictors in 0.5376930236816406 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591126\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591126\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591126\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591118\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591082\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591082\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591106\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591106\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591118\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591123\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591120\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591118\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591118\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591123\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591120\n",
      "         Iterations 6\n",
      "Processed  15 models on 8 predictors in 0.5597519874572754 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591106\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591105\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591105\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591102\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591035\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591035\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591106\n",
      "         Iterations 7\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m models_best \u001b[39m=\u001b[39m forward_selection()\n",
      "Cell \u001b[0;32mIn [25], line 9\u001b[0m, in \u001b[0;36mforward_selection\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m predictors \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(X\u001b[39m.\u001b[39mcolumns)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):    \n\u001b[0;32m----> 9\u001b[0m     models_best\u001b[39m.\u001b[39mloc[i] \u001b[39m=\u001b[39m forward(predictors)\n\u001b[1;32m     10\u001b[0m     predictors \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(models_best\u001b[39m.\u001b[39mloc[i][\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mindex[\u001b[39m1\u001b[39m:])\n\u001b[1;32m     12\u001b[0m toc \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn [24], line 12\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(predictors)\u001b[0m\n\u001b[1;32m      9\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m remaining_predictors:\n\u001b[0;32m---> 12\u001b[0m     results\u001b[39m.\u001b[39mappend(processSubset(predictors\u001b[39m+\u001b[39;49m[p]))\n\u001b[1;32m     14\u001b[0m \u001b[39m# Wrap everything up in a nice dataframe\u001b[39;00m\n\u001b[1;32m     15\u001b[0m models \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(results)\n",
      "Cell \u001b[0;32mIn [23], line 5\u001b[0m, in \u001b[0;36mprocessSubset\u001b[0;34m(predictor_subset)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocessSubset\u001b[39m(predictor_subset):\n\u001b[1;32m      4\u001b[0m     \u001b[39m# Fit model on feature_set and calculate R-squared\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     model \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39;49mlogit(\u001b[39m'\u001b[39;49m\u001b[39mHOME_TEAM_WINS~\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(predictor_subset),data \u001b[39m=\u001b[39;49m train)\u001b[39m.\u001b[39;49mfit()\n\u001b[1;32m      6\u001b[0m     ypred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(train)\n\u001b[1;32m      7\u001b[0m     y\u001b[39m=\u001b[39mtrain\u001b[39m.\u001b[39mHOME_TEAM_WINS\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/statsmodels/discrete/discrete_model.py:1983\u001b[0m, in \u001b[0;36mLogit.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m   1980\u001b[0m \u001b[39m@Appender\u001b[39m(DiscreteModel\u001b[39m.\u001b[39mfit\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m)\n\u001b[1;32m   1981\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, start_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnewton\u001b[39m\u001b[39m'\u001b[39m, maxiter\u001b[39m=\u001b[39m\u001b[39m35\u001b[39m,\n\u001b[1;32m   1982\u001b[0m         full_output\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, disp\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1983\u001b[0m     bnryfit \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(start_params\u001b[39m=\u001b[39;49mstart_params,\n\u001b[1;32m   1984\u001b[0m                           method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m   1985\u001b[0m                           maxiter\u001b[39m=\u001b[39;49mmaxiter,\n\u001b[1;32m   1986\u001b[0m                           full_output\u001b[39m=\u001b[39;49mfull_output,\n\u001b[1;32m   1987\u001b[0m                           disp\u001b[39m=\u001b[39;49mdisp,\n\u001b[1;32m   1988\u001b[0m                           callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m   1989\u001b[0m                           \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1991\u001b[0m     discretefit \u001b[39m=\u001b[39m LogitResults(\u001b[39mself\u001b[39m, bnryfit)\n\u001b[1;32m   1992\u001b[0m     \u001b[39mreturn\u001b[39;00m BinaryResultsWrapper(discretefit)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/statsmodels/discrete/discrete_model.py:230\u001b[0m, in \u001b[0;36mDiscreteModel.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# TODO: make a function factory to have multiple call-backs\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m mlefit \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(start_params\u001b[39m=\u001b[39;49mstart_params,\n\u001b[1;32m    231\u001b[0m                      method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    232\u001b[0m                      maxiter\u001b[39m=\u001b[39;49mmaxiter,\n\u001b[1;32m    233\u001b[0m                      full_output\u001b[39m=\u001b[39;49mfull_output,\n\u001b[1;32m    234\u001b[0m                      disp\u001b[39m=\u001b[39;49mdisp,\n\u001b[1;32m    235\u001b[0m                      callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    236\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m mlefit\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/statsmodels/base/model.py:579\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m     Hinv \u001b[39m=\u001b[39m cov_params_func(\u001b[39mself\u001b[39m, xopt, retvals)\n\u001b[1;32m    578\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnewton\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m full_output:\n\u001b[0;32m--> 579\u001b[0m     Hinv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49minv(\u001b[39m-\u001b[39;49mretvals[\u001b[39m'\u001b[39;49m\u001b[39mHessian\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m/\u001b[39m nobs\n\u001b[1;32m    580\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_hessian:\n\u001b[1;32m    581\u001b[0m     H \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhessian(xopt)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/numpy/linalg/linalg.py:552\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    550\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->D\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    551\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 552\u001b[0m ainv \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39;49minv(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[1;32m    553\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(ainv\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/numpy/linalg/linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSingular matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "models_best = forward_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models_best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39m# Predictors\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m     plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mBIC\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m best_sub_plots()\n",
      "Cell \u001b[0;32mIn [27], line 11\u001b[0m, in \u001b[0;36mbest_sub_plots\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# The argmax() function can be used to identify the location of the maximum point of a vector\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m plt\u001b[39m.\u001b[39mplot(models_best[\u001b[39m\"\u001b[39m\u001b[39mauc_roc\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     12\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39m# Predictors\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mauc_roc\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models_best' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def best_sub_plots():\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "    # Set up a 2x2 grid so we can look at 4 plots at once\n",
    "    plt.subplot(2, 2, 1)\n",
    "\n",
    "    # We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "    # The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "    plt.plot(models_best[\"auc_roc\"])\n",
    "    plt.xlabel('# Predictors')\n",
    "    plt.ylabel('auc_roc')\n",
    "\n",
    "    # We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n",
    "    aic = models_best.apply(lambda row: row[1].aic, axis=1)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(aic)\n",
    "    plt.plot(1+aic.argmin(), aic.min(), \"or\")\n",
    "    plt.xlabel('# Predictors')\n",
    "    plt.ylabel('AIC')\n",
    "\n",
    "    bic = models_best.apply(lambda row: row[1].bic, axis=1)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(bic)\n",
    "    plt.plot(1+bic.argmin(), bic.min(), \"or\")\n",
    "    plt.xlabel('# Predictors')\n",
    "    plt.ylabel('BIC')\n",
    "best_sub_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.671289\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.662699\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.668593\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.660647\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.667509\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.666973\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.668776\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.628480\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.639611\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.652314\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.663206\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.670464\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.671250\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.661490\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.663629\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.660561\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.657551\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.664434\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594509\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.643176\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.670645\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.653444\n",
      "         Iterations 5\n",
      "Processed  22 models on 1 predictors in 0.3212292194366455 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594509\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594252\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593689\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593859\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594222\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594063\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594394\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594152\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594152\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593456\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593787\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594509\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594024\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594463\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594463\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594417\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594472\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593974\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594490\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594266\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.594509\n",
      "         Iterations 5\n",
      "Processed  21 models on 2 predictors in 0.3768749237060547 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593456\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593439\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592938\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593347\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593327\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592771\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593296\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593444\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593444\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593132\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593443\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593083\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593428\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593455\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593046\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593452\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592655\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593132\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593179\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593437\n",
      "         Iterations 6\n",
      "Processed  20 models on 3 predictors in 0.37182116508483887 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592655\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592431\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591893\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592623\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592515\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592571\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592571\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592655\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592655\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592502\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592652\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592405\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592651\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592622\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591628\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592625\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592502\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592490\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592646\n",
      "         Iterations 6\n",
      "Processed  19 models on 4 predictors in 0.43126893043518066 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591628\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591608\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591608\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591490\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591617\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591538\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591538\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591586\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591586\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591598\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591623\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591529\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591623\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591474\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591506\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591598\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591595\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591566\n",
      "         Iterations 6\n",
      "Processed  18 models on 5 predictors in 0.5831878185272217 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591490\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591490\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591490\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591469\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591444\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591444\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591413\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591413\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591441\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591489\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591389\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591477\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591318\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591469\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591441\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591447\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591436\n",
      "         Iterations 6\n",
      "Processed  17 models on 6 predictors in 0.5825190544128418 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591318\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591314\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591314\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591310\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591279\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591279\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591291\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591291\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591300\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591311\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591126\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591314\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591310\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591300\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591253\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591314\n",
      "         Iterations 6\n",
      "Processed  16 models on 7 predictors in 0.5412688255310059 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591126\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591126\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591126\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591118\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591082\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591082\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591106\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591106\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591118\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591123\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591120\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591118\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591118\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591123\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591120\n",
      "         Iterations 6\n",
      "Processed  15 models on 8 predictors in 0.582064151763916 seconds.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591106\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591105\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591105\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591102\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591035\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591035\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591106\n",
      "         Iterations 7\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m models_best \u001b[39m=\u001b[39m forward_selection()\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbest_sub_plots\u001b[39m():\n",
      "Cell \u001b[0;32mIn [13], line 8\u001b[0m, in \u001b[0;36mforward_selection\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m predictors \u001b[39m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(X\u001b[39m.\u001b[39mcolumns)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):    \n\u001b[0;32m----> 8\u001b[0m     models_best\u001b[39m.\u001b[39mloc[i] \u001b[39m=\u001b[39m forward(predictors)\n\u001b[1;32m      9\u001b[0m     predictors \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(models_best\u001b[39m.\u001b[39mloc[i][\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mindex[\u001b[39m1\u001b[39m:])\n\u001b[1;32m     11\u001b[0m toc \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn [12], line 12\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(predictors)\u001b[0m\n\u001b[1;32m      9\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m remaining_predictors:\n\u001b[0;32m---> 12\u001b[0m     results\u001b[39m.\u001b[39mappend(processSubset(predictors\u001b[39m+\u001b[39;49m[p]))\n\u001b[1;32m     14\u001b[0m \u001b[39m# Wrap everything up in a nice dataframe\u001b[39;00m\n\u001b[1;32m     15\u001b[0m models \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(results)\n",
      "Cell \u001b[0;32mIn [10], line 5\u001b[0m, in \u001b[0;36mprocessSubset\u001b[0;34m(predictor_subset)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocessSubset\u001b[39m(predictor_subset):\n\u001b[1;32m      4\u001b[0m     \u001b[39m# Fit model on feature_set and calculate R-squared\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     model \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39;49mlogit(\u001b[39m'\u001b[39;49m\u001b[39mHOME_TEAM_WINS~\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(predictor_subset),data \u001b[39m=\u001b[39;49m train)\u001b[39m.\u001b[39;49mfit()\n\u001b[1;32m      6\u001b[0m     ypred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(train)\n\u001b[1;32m      7\u001b[0m     y\u001b[39m=\u001b[39mtrain\u001b[39m.\u001b[39mHOME_TEAM_WINS\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/statsmodels/discrete/discrete_model.py:1983\u001b[0m, in \u001b[0;36mLogit.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m   1980\u001b[0m \u001b[39m@Appender\u001b[39m(DiscreteModel\u001b[39m.\u001b[39mfit\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m)\n\u001b[1;32m   1981\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, start_params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnewton\u001b[39m\u001b[39m'\u001b[39m, maxiter\u001b[39m=\u001b[39m\u001b[39m35\u001b[39m,\n\u001b[1;32m   1982\u001b[0m         full_output\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, disp\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1983\u001b[0m     bnryfit \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(start_params\u001b[39m=\u001b[39;49mstart_params,\n\u001b[1;32m   1984\u001b[0m                           method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m   1985\u001b[0m                           maxiter\u001b[39m=\u001b[39;49mmaxiter,\n\u001b[1;32m   1986\u001b[0m                           full_output\u001b[39m=\u001b[39;49mfull_output,\n\u001b[1;32m   1987\u001b[0m                           disp\u001b[39m=\u001b[39;49mdisp,\n\u001b[1;32m   1988\u001b[0m                           callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m   1989\u001b[0m                           \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1991\u001b[0m     discretefit \u001b[39m=\u001b[39m LogitResults(\u001b[39mself\u001b[39m, bnryfit)\n\u001b[1;32m   1992\u001b[0m     \u001b[39mreturn\u001b[39;00m BinaryResultsWrapper(discretefit)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/statsmodels/discrete/discrete_model.py:230\u001b[0m, in \u001b[0;36mDiscreteModel.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, callback, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# TODO: make a function factory to have multiple call-backs\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m mlefit \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(start_params\u001b[39m=\u001b[39;49mstart_params,\n\u001b[1;32m    231\u001b[0m                      method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    232\u001b[0m                      maxiter\u001b[39m=\u001b[39;49mmaxiter,\n\u001b[1;32m    233\u001b[0m                      full_output\u001b[39m=\u001b[39;49mfull_output,\n\u001b[1;32m    234\u001b[0m                      disp\u001b[39m=\u001b[39;49mdisp,\n\u001b[1;32m    235\u001b[0m                      callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    236\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m mlefit\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/statsmodels/base/model.py:579\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m     Hinv \u001b[39m=\u001b[39m cov_params_func(\u001b[39mself\u001b[39m, xopt, retvals)\n\u001b[1;32m    578\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnewton\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m full_output:\n\u001b[0;32m--> 579\u001b[0m     Hinv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49minv(\u001b[39m-\u001b[39;49mretvals[\u001b[39m'\u001b[39;49m\u001b[39mHessian\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m/\u001b[39m nobs\n\u001b[1;32m    580\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_hessian:\n\u001b[1;32m    581\u001b[0m     H \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhessian(xopt)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/numpy/linalg/linalg.py:552\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    550\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->D\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    551\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 552\u001b[0m ainv \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39;49minv(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[1;32m    553\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(ainv\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/numpy/linalg/linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSingular matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yui/Library/Python/3.8/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>HOME_TEAM_WINS</th>\n",
       "      <th>HOME_TEAM_PTS_AVG</th>\n",
       "      <th>AWAY_TEAM_PTS_AVG</th>\n",
       "      <th>HOME_TEAM_AST_AVG</th>\n",
       "      <th>AWAY_TEAM_AST_AVG</th>\n",
       "      <th>HOME_TEAM_REB_AVG</th>\n",
       "      <th>AWAY_TEAM_REB_AVG</th>\n",
       "      <th>HOME_TEAM_WIN_PCT</th>\n",
       "      <th>...</th>\n",
       "      <th>DIFF_AST_AVG DIFF_REB_AVG</th>\n",
       "      <th>DIFF_AST_AVG DIFF_WIN_PCT</th>\n",
       "      <th>DIFF_AST_AVG DIFF_FG_PCT</th>\n",
       "      <th>DIFF_AST_AVG DIFF_FT_PCT</th>\n",
       "      <th>DIFF_REB_AVG DIFF_WIN_PCT</th>\n",
       "      <th>DIFF_REB_AVG DIFF_FG_PCT</th>\n",
       "      <th>DIFF_REB_AVG DIFF_FT_PCT</th>\n",
       "      <th>DIFF_WIN_PCT DIFF_FG_PCT</th>\n",
       "      <th>DIFF_WIN_PCT DIFF_FT_PCT</th>\n",
       "      <th>DIFF_FG_PCT DIFF_FT_PCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5881.0</td>\n",
       "      <td>90.437500</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>19.312500</td>\n",
       "      <td>20.875000</td>\n",
       "      <td>40.187500</td>\n",
       "      <td>39.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>-0.027980</td>\n",
       "      <td>-0.037426</td>\n",
       "      <td>-0.039762</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.008637</td>\n",
       "      <td>0.009176</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.002254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5882.0</td>\n",
       "      <td>99.933333</td>\n",
       "      <td>97.444444</td>\n",
       "      <td>21.666667</td>\n",
       "      <td>18.277778</td>\n",
       "      <td>46.266667</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.066161</td>\n",
       "      <td>-0.048633</td>\n",
       "      <td>0.008890</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>-0.006437</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>-0.000626</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5883.0</td>\n",
       "      <td>100.647059</td>\n",
       "      <td>92.526316</td>\n",
       "      <td>21.470588</td>\n",
       "      <td>20.736842</td>\n",
       "      <td>43.529412</td>\n",
       "      <td>42.473684</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411832</td>\n",
       "      <td>0.030999</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>-0.027419</td>\n",
       "      <td>0.011454</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>-0.010132</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>-0.000763</td>\n",
       "      <td>-0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5884.0</td>\n",
       "      <td>98.411765</td>\n",
       "      <td>99.352941</td>\n",
       "      <td>21.529412</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>47.352941</td>\n",
       "      <td>39.470588</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.391003</td>\n",
       "      <td>-0.162747</td>\n",
       "      <td>0.294893</td>\n",
       "      <td>-0.720076</td>\n",
       "      <td>0.003644</td>\n",
       "      <td>-0.006602</td>\n",
       "      <td>0.016121</td>\n",
       "      <td>-0.000772</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>-0.003418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5885.0</td>\n",
       "      <td>91.888889</td>\n",
       "      <td>97.421053</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>41.722222</td>\n",
       "      <td>39.894737</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026718</td>\n",
       "      <td>-0.063305</td>\n",
       "      <td>-0.117953</td>\n",
       "      <td>-0.002437</td>\n",
       "      <td>-0.000506</td>\n",
       "      <td>-0.000944</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 254 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  Unnamed: 0  HOME_TEAM_WINS  HOME_TEAM_PTS_AVG  AWAY_TEAM_PTS_AVG  \\\n",
       "0  1.0      5881.0       90.437500          91.250000          19.312500   \n",
       "1  1.0      5882.0       99.933333          97.444444          21.666667   \n",
       "2  1.0      5883.0      100.647059          92.526316          21.470588   \n",
       "3  1.0      5884.0       98.411765          99.352941          21.529412   \n",
       "4  1.0      5885.0       91.888889          97.421053          16.000000   \n",
       "\n",
       "   HOME_TEAM_AST_AVG  AWAY_TEAM_AST_AVG  HOME_TEAM_REB_AVG  AWAY_TEAM_REB_AVG  \\\n",
       "0          20.875000          40.187500          39.375000           0.250000   \n",
       "1          18.277778          46.266667          44.000000           0.800000   \n",
       "2          20.736842          43.529412          42.473684           0.705882   \n",
       "3          19.764706          47.352941          39.470588           0.470588   \n",
       "4          19.000000          41.722222          39.894737           0.277778   \n",
       "\n",
       "   HOME_TEAM_WIN_PCT  ...  DIFF_AST_AVG DIFF_REB_AVG  \\\n",
       "0           0.437500  ...                  -0.152344   \n",
       "1           0.500000  ...                   0.680000   \n",
       "2           0.315789  ...                   0.411832   \n",
       "3           0.647059  ...                  -1.391003   \n",
       "4           0.263158  ...                   0.026718   \n",
       "\n",
       "   DIFF_AST_AVG DIFF_WIN_PCT  DIFF_AST_AVG DIFF_FG_PCT  \\\n",
       "0                  -0.027980                 -0.037426   \n",
       "1                   0.066161                 -0.048633   \n",
       "2                   0.030999                  0.000742   \n",
       "3                  -0.162747                  0.294893   \n",
       "4                  -0.063305                 -0.117953   \n",
       "\n",
       "   DIFF_AST_AVG DIFF_FT_PCT  DIFF_REB_AVG DIFF_WIN_PCT  \\\n",
       "0                 -0.039762                   0.006457   \n",
       "1                  0.008890                   0.008757   \n",
       "2                 -0.027419                   0.011454   \n",
       "3                 -0.720076                   0.003644   \n",
       "4                 -0.002437                  -0.000506   \n",
       "\n",
       "   DIFF_REB_AVG DIFF_FG_PCT  DIFF_REB_AVG DIFF_FT_PCT  \\\n",
       "0                  0.008637                  0.009176   \n",
       "1                 -0.006437                  0.001177   \n",
       "2                  0.000274                 -0.010132   \n",
       "3                 -0.006602                  0.016121   \n",
       "4                 -0.000944                 -0.000019   \n",
       "\n",
       "   DIFF_WIN_PCT DIFF_FG_PCT  DIFF_WIN_PCT DIFF_FT_PCT  DIFF_FG_PCT DIFF_FT_PCT  \n",
       "0                  0.001586                  0.001685                 0.002254  \n",
       "1                 -0.000626                  0.000114                -0.000084  \n",
       "2                  0.000021                 -0.000763                -0.000018  \n",
       "3                 -0.000772                  0.001886                -0.003418  \n",
       "4                  0.002236                  0.000046                 0.000086  \n",
       "\n",
       "[5 rows x 254 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X.copy()\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "trans = PolynomialFeatures(include_bias = True, interaction_only = True)\n",
    "data = trans.fit_transform(X_train)\n",
    "\n",
    "X_trans_train = pd.DataFrame(data, columns = trans.get_feature_names(train.columns))\n",
    "X_trans_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/n_ksdn3d24l74_zsdlmr7drh0000gn/T/ipykernel_9387/2460516403.py:20: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for k in tnrange(1,len(X_trans_train.columns) + 1, desc = 'Loop...'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edd01e188c4489eb1dd2b82773a0e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loop...:   0%|          | 0/254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m tnrange(\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(X_trans_train\u001b[39m.\u001b[39mcolumns) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, desc \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLoop...\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m     \u001b[39m#Looping over all possible combinations: from 11 choose k\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[39mfor\u001b[39;00m combo \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcombinations(X_trans_train\u001b[39m.\u001b[39mcolumns,k):\n\u001b[0;32m---> 24\u001b[0m         tmp_result \u001b[39m=\u001b[39m fit_logistic(X_trans_train[\u001b[39mlist\u001b[39;49m(combo)],y_train)   \u001b[39m#Store temp result \u001b[39;00m\n\u001b[1;32m     25\u001b[0m         roc_auc_list\u001b[39m.\u001b[39mappend(tmp_result[\u001b[39m0\u001b[39m])                  \u001b[39m#Append lists\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         accuracy_list\u001b[39m.\u001b[39mappend(tmp_result[\u001b[39m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn [30], line 14\u001b[0m, in \u001b[0;36mfit_logistic\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_logistic\u001b[39m(X,Y):\n\u001b[1;32m     12\u001b[0m     \u001b[39m#Fit linear regression model and return RSS and R squared values\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     sklearn_model \u001b[39m=\u001b[39m LogisticRegression( max_iter \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     sklearn_model\u001b[39m.\u001b[39;49mfit(X,Y)\n\u001b[1;32m     15\u001b[0m     accuracy \u001b[39m=\u001b[39m sklearn_model\u001b[39m.\u001b[39mscore(X, Y)\n\u001b[1;32m     16\u001b[0m     roc_auc \u001b[39m=\u001b[39m roc_auc_score(Y, sklearn_model\u001b[39m.\u001b[39mpredict(X))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:1233\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1231\u001b[0m     n_threads \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1233\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, prefer\u001b[39m=\u001b[39;49mprefer)(\n\u001b[1;32m   1234\u001b[0m     path_func(\n\u001b[1;32m   1235\u001b[0m         X,\n\u001b[1;32m   1236\u001b[0m         y,\n\u001b[1;32m   1237\u001b[0m         pos_class\u001b[39m=\u001b[39;49mclass_,\n\u001b[1;32m   1238\u001b[0m         Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[1;32m   1239\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[1;32m   1240\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m   1241\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m   1242\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1243\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[1;32m   1244\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[1;32m   1245\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m   1246\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m   1247\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1248\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m   1249\u001b[0m         coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[1;32m   1250\u001b[0m         penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[1;32m   1251\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[1;32m   1252\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1253\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[1;32m   1254\u001b[0m     )\n\u001b[1;32m   1255\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef)\n\u001b[1;32m   1256\u001b[0m )\n\u001b[1;32m   1258\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_logistic.py:436\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    432\u001b[0m l2_reg_strength \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C\n\u001b[1;32m    433\u001b[0m iprint \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m101\u001b[39m][\n\u001b[1;32m    434\u001b[0m     np\u001b[39m.\u001b[39msearchsorted(np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]), verbose)\n\u001b[1;32m    435\u001b[0m ]\n\u001b[0;32m--> 436\u001b[0m opt_res \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[1;32m    437\u001b[0m     func,\n\u001b[1;32m    438\u001b[0m     w0,\n\u001b[1;32m    439\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    440\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    441\u001b[0m     args\u001b[39m=\u001b[39;49m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[1;32m    442\u001b[0m     options\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint, \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: tol, \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_iter},\n\u001b[1;32m    443\u001b[0m )\n\u001b[1;32m    444\u001b[0m n_iter_i \u001b[39m=\u001b[39m _check_optimize_result(\n\u001b[1;32m    445\u001b[0m     solver,\n\u001b[1;32m    446\u001b[0m     opt_res,\n\u001b[1;32m    447\u001b[0m     max_iter,\n\u001b[1;32m    448\u001b[0m     extra_warning_msg\u001b[39m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    450\u001b[0m w0, loss \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_minimize.py:699\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    697\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    700\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    701\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    702\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    703\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_lbfgsb_py.py:308\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         iprint \u001b[39m=\u001b[39m disp\n\u001b[0;32m--> 308\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[39m=\u001b[39;49mjac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    309\u001b[0m                               bounds\u001b[39m=\u001b[39;49mnew_bounds,\n\u001b[1;32m    310\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step)\n\u001b[1;32m    312\u001b[0m func_and_grad \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mfun_and_grad\n\u001b[1;32m    314\u001b[0m fortran_int \u001b[39m=\u001b[39m _lbfgsb\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mintvar\u001b[39m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_optimize.py:263\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    259\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[1;32m    261\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m    264\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[1;32m    266\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[1;32m    160\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m callable(grad):\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m     \u001b[39m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/linear_model/_linear_loss.py:189\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads)\u001b[0m\n\u001b[1;32m    186\u001b[0m n_dof \u001b[39m=\u001b[39m n_features \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept)\n\u001b[1;32m    187\u001b[0m weights, intercept, raw_prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_w_intercept_raw(coef, X)\n\u001b[0;32m--> 189\u001b[0m loss, grad_per_sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_loss\u001b[39m.\u001b[39;49mloss_gradient(\n\u001b[1;32m    190\u001b[0m     y_true\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    191\u001b[0m     raw_prediction\u001b[39m=\u001b[39;49mraw_prediction,\n\u001b[1;32m    192\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    193\u001b[0m     n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[1;32m    194\u001b[0m )\n\u001b[1;32m    195\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39msum()\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_loss\u001b[39m.\u001b[39mis_multiclass:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/sklearn/_loss/loss.py:257\u001b[0m, in \u001b[0;36mBaseLoss.loss_gradient\u001b[0;34m(self, y_true, raw_prediction, sample_weight, loss_out, gradient_out, n_threads)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     sample_weight \u001b[39m=\u001b[39m ReadonlyArrayWrapper(sample_weight)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcloss\u001b[39m.\u001b[39;49mloss_gradient(\n\u001b[1;32m    258\u001b[0m     y_true\u001b[39m=\u001b[39;49my_true,\n\u001b[1;32m    259\u001b[0m     raw_prediction\u001b[39m=\u001b[39;49mraw_prediction,\n\u001b[1;32m    260\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    261\u001b[0m     loss_out\u001b[39m=\u001b[39;49mloss_out,\n\u001b[1;32m    262\u001b[0m     gradient_out\u001b[39m=\u001b[39;49mgradient_out,\n\u001b[1;32m    263\u001b[0m     n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[1;32m    264\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Importing tqdm for the progress bar\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "y_train = y.copy()\n",
    "\n",
    "k = 11\n",
    "roc_auc_list, accuracy_list, feature_list = [],[], []\n",
    "numb_features = []\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def fit_logistic(X,Y):\n",
    "    #Fit linear regression model and return RSS and R squared values\n",
    "    sklearn_model = LogisticRegression( max_iter = 1000)\n",
    "    sklearn_model.fit(X,Y)\n",
    "    accuracy = sklearn_model.score(X, Y)\n",
    "    roc_auc = roc_auc_score(Y, sklearn_model.predict(X))\n",
    "    return accuracy, roc_auc\n",
    "\n",
    "#Looping over k = 1 to k = 11 features in X\n",
    "for k in tnrange(1,len(X_trans_train.columns) + 1, desc = 'Loop...'):\n",
    "\n",
    "    #Looping over all possible combinations: from 11 choose k\n",
    "    for combo in itertools.combinations(X_trans_train.columns,k):\n",
    "        tmp_result = fit_logistic(X_trans_train[list(combo)],y_train)   #Store temp result \n",
    "        roc_auc_list.append(tmp_result[0])                  #Append lists\n",
    "        accuracy_list.append(tmp_result[1])\n",
    "        feature_list.append(combo)\n",
    "        numb_features.append(len(combo))   \n",
    "\n",
    "#Store in DataFrame\n",
    "df = pd.DataFrame({'numb_features': numb_features,'ROC_AUC': roc_auc_list, 'accuracy':accuracy_list,'features':feature_list})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
